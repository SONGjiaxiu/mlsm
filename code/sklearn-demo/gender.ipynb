{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# In this example, I walk through how to use sklearn to classify users into male or female\n",
      "# based on their user description.\n",
      "\n",
      "# First, we need to get some tweets in JSON format.\n",
      "# Create a tweets.json file with something like:\n",
      "# twitter-curl --query \"track=obama\" > tweets.json\n",
      "# This will query twitter for tweets containing the word obama.\n",
      "\n",
      "# Now, we'll parse that file into a list of (name, description) tuples.\n",
      "import json\n",
      "import io\n",
      "# open the json file\n",
      "fp = io.open('tweets.json', mode='rt', encoding='utf8')\n",
      "# read the names and description fields from each tweet.\n",
      "data = []\n",
      "for line in fp:\n",
      "    js = json.loads(line)  # parse into a JSON object.\n",
      "    name = js['user']['name']\n",
      "    description = js['user']['description']\n",
      "    if name and description:  # if fields aren't blank\n",
      "        data.append((name.lower(), description.lower()))\n",
      "print 'read', len(data), 'users'\n",
      "print 'example:', data[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 7681 users\n",
        "example: (u'conservative chick', u\"i'm that evil conservative obama warned you about...               pro-palin #conservative, #pro-life,\")\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, we need to label them as male or female. \n",
      "# To do that, we get the top 500 male/female names from census\n",
      "import requests  # This is a very handy library for html requests.\n",
      "males = requests.get('http://www.census.gov/genealogy/www/data/1990surnames/dist.male.first').text.split('\\n')\n",
      "males = [m.split()[0].lower() for m in males[:500]]  # lower case and take top 500\n",
      "print 'first male is', males[0]\n",
      "females = requests.get('http://www.census.gov/genealogy/www/data/1990surnames/dist.female.first').text.split('\\n')\n",
      "females = [f.split()[0].lower() for f in females[:500]]  # lower case and take top 500\n",
      "print 'first female is', females[0]\n",
      "\n",
      "# Remove ambiguous names (those that appear on both lists)\n",
      "# Note that the plus operator is overloaded to mean concatentation for lists.\n",
      "ambiguous = [f for f in females + males if f in males and f in females]\n",
      "print 'ambiguous is', ambiguous[0]\n",
      "males = [m for m in males if m not in ambiguous]\n",
      "females = [f for f in females if f not in ambiguous]\n",
      "print 'got', len(males), 'males and', len(females), 'females'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "first male is james\n",
        "first female is"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " mary\n",
        "ambiguous is jean\n",
        "got 473 males and 473 females\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sort male, female users\n",
      "male_users = [d for d in data if len(d[0].split()) > 0 and d[0].split()[0] in males]\n",
      "print len(male_users), 'males'\n",
      "print male_users[0]\n",
      "female_users = [d for d in data if len(d[0].split()) > 0 and d[0].split()[0] in females]\n",
      "print len(female_users), 'females'\n",
      "print female_users[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1637 males\n",
        "(u'daniel john sobieski', u\"editorial writer, investor's business daily, #reagan #conservative, somewhere to the right of attila the hun #catholic #prolife #tcot #teaparty #nra\")\n",
        "780"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " females\n",
        "(u'jessica cranor ', u'lame, nerdy, brilliant, fantastic')\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make target vector. Female=1, Male=0\n",
      "import numpy as np\n",
      "y = np.array([0.] * len(male_users) + [1.] * len(female_users))\n",
      "data = [d[1] for d in male_users + female_users]\n",
      "print 'first label=', y[0]\n",
      "print 'first description=', data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "first label= 0.0\n",
        "first description= editorial writer, investor's business daily, #reagan #conservative, somewhere to the right of attila the hun #catholic #prolife #tcot #teaparty #nra\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert descriptions into feature vectors.\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vec = CountVectorizer()\n",
      "X = vec.fit_transform(data)\n",
      "print data[0],'\\nis transformed into the sparse vector\\n', X[0]\n",
      "print 'the word THE is mapped to index', vec.vocabulary_['the']\n",
      "print 'there are', len(vec.vocabulary_), 'unique features'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "editorial writer, investor's business daily, #reagan #conservative, somewhere to the right of attila the hun #catholic #prolife #tcot #teaparty #nra \n",
        "is transformed into the sparse vector\n",
        "  (0, 662)\t1\n",
        "  (0, 1142)\t1\n",
        "  (0, 1274)\t1\n",
        "  (0, 1633)\t1\n",
        "  (0, 1846)\t1\n",
        "  (0, 2248)\t1\n",
        "  (0, 3396)\t1\n",
        "  (0, 3625)\t1\n",
        "  (0, 4803)\t1\n",
        "  (0, 4880)\t1\n",
        "  (0, 5480)\t1\n",
        "  (0, 5663)\t1\n",
        "  (0, 5863)\t1\n",
        "  (0, 6330)\t1\n",
        "  (0, 6663)\t1\n",
        "  (0, 6693)\t1\n",
        "  (0, 6757)\t2\n",
        "  (0, 6851)\t1\n",
        "  (0, 7522)\t1\n",
        "the word THE is mapped to index 6757\n",
        "there are 7662 unique features\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute cross validation accuracy\n",
      "from sklearn import cross_validation\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "clf = LogisticRegression()\n",
      "print 'avg accuracy=%.3f' % np.average(cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy=0.807\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try Naive Bayes\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "clf = MultinomialNB()\n",
      "print 'avg accuracy=%.3f' % np.average(cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy=0.752\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try adding bigrams\n",
      "vec = CountVectorizer(ngram_range=(1,2))\n",
      "X = vec.fit_transform(data)\n",
      "print 'there are', len(vec.vocabulary_), 'unique features'\n",
      "print 'ten feature examples:', vec.vocabulary_.keys()[:10]\n",
      "from sklearn import cross_validation\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "clf = LogisticRegression()\n",
      "print 'avg accuracy with bigrams=%.3f' % np.average(cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "there are 27322 unique features\n",
        "ten feature examples: [u'16 33', u'my hobbies', u'loving faithful', u'member aft', u'twitter thing', u'whizzy_walexzy', u'rush daily', u'lom', u'se it', u'expect the']\n",
        "avg accuracy with bigrams=0.815"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print top feature weights for female\n",
      "clf = LogisticRegression()\n",
      "clf.fit(X, y)  # fit on all data\n",
      "top_indices = clf.coef_[0].argsort()[::-1] # sort in decreasing order\n",
      "# reverse the alphabet to map from idx->word\n",
      "vocab_r = dict((idx, word) for word, idx in vec.vocabulary_.iteritems())\n",
      "print 'female words:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], clf.coef_[0][idx]) for idx in top_indices[:20]])\n",
      "top_indices = clf.coef_[0].argsort() # sort in increasing order\n",
      "print '\\n\\nmale words:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], clf.coef_[0][idx]) for idx in top_indices[:20]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "female words:\n",
        "mom=2.050\n",
        "mother=1.767\n",
        "christian=1.034\n",
        "wife=0.976\n",
        "grandmother=0.962\n",
        "girl=0.959\n",
        "lover=0.863\n",
        "fighter=0.848\n",
        "kind=0.736\n",
        "loving=0.722\n",
        "guns and=0.710\n",
        "one=0.697\n",
        "children=0.695\n",
        "lover of=0.693\n",
        "woman=0.671\n",
        "loves=0.659\n",
        "just don=0.643\n",
        "communications=0.643\n",
        "indian=0.642\n",
        "ayeeee=0.642\n",
        "\n",
        "\n",
        "male words:\n",
        "father=-1.423\n",
        "sports=-1.182\n",
        "husband=-1.146\n",
        "of the=-0.873\n",
        "veteran=-0.869\n",
        "love jesus=-0.806\n",
        "radio=-0.726\n",
        "dad=-0.704\n",
        "man=-0.703\n",
        "boy=-0.691\n",
        "jesus wife=-0.687\n",
        "consurvative love=-0.687\n",
        "consurvative=-0.687\n",
        "army=-0.644\n",
        "editor=-0.641\n",
        "guy=-0.639\n",
        "liberty=-0.638\n",
        "my wife=-0.614\n",
        "your=-0.606\n",
        "entrepreneur=-0.585\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use PCA to reduce the dimensionality of X to only 2 dimensions,\n",
      "# then compute cross-validation accuracy of resulting data X2.\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "X2 = pca.fit_transform(X.toarray())\n",
      "print 'first document with reduced representation'\n",
      "print X2[0]\n",
      "dim1 = pca.components_[0]\n",
      "print 'first PCA dimension (eigenvector):', dim1\n",
      "top_indices = dim1.argsort()[::-1]\n",
      "print 'top words of first dimension:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], dim1[idx]) for idx in top_indices[:20]])\n",
      "dim2 = pca.components_[1]\n",
      "print 'second PCA dimension (eigenvector):', dim2\n",
      "top_indices = dim2.argsort()[::-1]\n",
      "print 'top words of second dimension:\\n', '\\n'.join(['%s=%.3f' % (vocab_r[idx], dim2[idx]) for idx in top_indices[:20]])\n",
      "print 'avg accuracy using only 2 dimensions=%.3f' % np.average(cross_validation.cross_val_score(clf, X2, y, cv=5, scoring='accuracy'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}