Overview

This paper argues that several current papers advocating the use of Twitter as a predictive model for elections are biased toward their hypothesis and overlook key features. The author proceeds to provide a survey of several papers, from what is proclaimed to be the first paper dealing with mood analysis on Twitter data to papers published the same year as this work.



Survey

The paper provides summaries and short evaluations regarding the viability of the results of seventeen papers on topics relating to Twitter data analysis and predictions. These papers are presented chronologically to provide some context, and paper responses are directly mentioned as being linked.



Assumptions

The main assumption of this work is that there is not a paper published at the time of writing hat is able to satisfy, or at least address, all the author’s suggestions for improvement presented in the introduction.



Synthesis

It seems off-putting for a survey writer to have such a strong, negative opinion about the work he/she is surveying. By starting the papers with detailed descriptions of the failures of the papers covering the subject, the reader assumes that all papers discussed do not consider the points the author mentions, and thus are of low quality. It also calls in to question the selection of these papers by the author, as papers that do not violate the author’s recommendations may be ignored to help further the argument. Although these points are negated somewhat by the well-written survey section, it may be more pertinent to leave the bias to the concluding section of a survey to help illustrate that the formation of the author’s opinion occurred after selecting and surveying the work and not before, as should be the case in a “balanced survey”.

The description of the papers seems to deal more with the general purpose of the paper and the relationship between other future and previous works. As such, the majority of the survey is useful, but there are some particularly uninformed and biased phrases (“the arguments…are not compelling enough”) that throw doubt on the validity of the analysis and selection even without the leading introduction. 

The points that are mentioned in the introduction are rarely mentioned in the analysis of the surveyed works, so that there is little indication as to whether the original work’s author mentioned and attempted to address the survey writer’s issues. This might have some correlation that several of the papers are only related to the task at hand (gender identification from tweets, information credibility, Truthy project, etc). Even in those papers, it would be beneficial to note something to the effect of which papers mentioned afterward, if any, cite this tangentially related work as reference.



Related Papers

Although I was unable to find other surveys, I did look at two papers I thought could be relevant to the author’s topic but were not included in the survey.

Conover, M.D., Ratkiewicz, J., Francisco, M., Gonçalves, B., Flammini, A., and Menczer, F. 2011. Political Polarization on Twitter. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.

This document focused more on the connections between Twitter users from different political parties, rather than the results that such connections may imply when an election is considered.

Lampos, V. 2012. On voting intentions inference from Twitter content: a case study on UK 2010 General Election. arXiv:1204.0423

In this paper about a work-in-progress using multiple methods on tweets occurring over a long duration, the author is able to make predictions with comparable deviations as those which occur between polls. I would be interested in the survey author’s reaction to this paper, as it seems to be a reasonably thorough analysis.
