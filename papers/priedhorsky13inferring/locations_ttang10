Overview:
This paper proposed a scalable, content-based approach to estimate the location of tweets via using gaussian mixture models. They also set some metrics of accuracy, precision and calibration for evaluation. They tested on multi-language tweets and concludes results about duration, temporal gap, range of geo-tagged tweets through comparing different experiments.

Algorithm:
GMM-Opt-ID
Gaussian-Opt-ID
GMM-Qpr-Covar-Sum-Prod
GMM-Err-SAE4

Hypothesis:
The novel, simple, and scalable GMM-based approach produces well-calibrated estimates.
There is subtle effect of temporal gaps between training and testing set.
The user location string and time zone fields provide the strongest signals, tweet text and user language are weaker but important to offer an estimate for all test tweets, and user description has essentially no location value.

Data:
They made use of twitter API and collected  an approximately continuous 1% sample of all global tweets from January 25, 2012 to January 23, 2013. These yielded a total of approximately 13 million geotagged tweets. They tokenized the message text (tx), user description (ds), and user location (lo) fields, which were free-text, into bigrams by splitting on Unicode character category and script boundaries and then further subdividing bigrams which appear to be Japanese using the TinySegmenter algorithm. For the language (ln) and time zone (tz) fields, which were selected from a set of options, they formed n-grams by simply removing whitespace and punctuation and converting to lowercase.

Experiments:
