Overview
In this paper, they describe automatic methods for extracting information from microblog posts. Specifically, they focus on extracting valuable “information nuggets”, brief, self-contained information items relevant to disaster response.

Algorithm
They use the binary features to classify the data sets.
For caution tweets, the following information was extracted: 1) Location references; 2) Time references; 3) Caution/Advice; 4) Source; 5) Type of Caution.
For Casualty tweets, the following was extracted: 1)Location references; 2)Time references; 3)Number of Casualties; 4) Damaged Object; 5)Source; 6) Type of Casualty/Damage.

Hypothesis
The Joplin dataset are reliable.

Data
The dataset consists of tweets posted during the Joplin 2011 tornado that struck Joplin, Missouri in the late afternoon of Sunday, May 22, 2011. The dataset was originally constructed by researchers at the University of Colorado at Boulder5. The 206,764 unique tweets were selected by monitoring the Twitter Streaming API using the hashtag #joplin a few hours after the tornado hit.

Experiments
1.	Manual classification and extraction with crowdsourcing
  1.	Filtering information messages
  2.	Classifying messages by type
  3.	Classifying messages by sub-type and extracting information nuggets
2.	Automatic classification and extracting with machine learning
  Classification
    1.	Filtering informative tweets
    2.	Identifying Eye-Witness tweets
  Information Extraction
    1.	Caution and advice nuggets
    2.	Casualty and damage nuggets
    3.	Donation and offer nuggets
    4.	Information source nuggets

Results
All the tweets can be successfully classified and the information can also be extracted from the tweets.

Assumptions
They assumed that all the methods cited by authors are reliable.

Synthesis
I think the author proved a useful method to classify and extract information nuggets from microblogging messages during disaster time. Some other points can be explored deeper, such as how to cooperate with the local government to control the spread of the disaster.

Related paper
Mark Craven and Johan Kumlien, Constructing Biological Knowledge Bases by Extracting Information from Text Sources, They describe two learning methods that we have applied to this task -- a statistical text classification method, and a relational learning method -- and our initial experiments in learning such information-extraction routines.

P. S. Jacobs, Lisa F. Rau, SCISOR: extracting information from on-line news, The future of natural language text processing is examined in the SCISOR prototype. Drawing on artificial intelligence techniques, and applying them to financial news items, this powerful tool illustrates some of the future benefits of natural language analysis through a combination of bottom-up and top-down processing.
